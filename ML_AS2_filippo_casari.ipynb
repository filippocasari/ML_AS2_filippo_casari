{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/filippocasari/ML_AS2_filippo_casari/blob/main/ML_AS2_filippo_casari.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1kKJk_rJC34"
      },
      "source": [
        "# Machine Learning 2021/2022\n",
        "## Assignment 2: Probabilistic Modeling, Unsupervised Learning, Reinforcement Learning\n",
        "Deadline: 20th of December 2021 9pm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLtJ0vlKJReh"
      },
      "source": [
        "First name: Filippo  \n",
        "Last name: Casari"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8xajf5MJ8Ud"
      },
      "source": [
        "## About this assignment\n",
        "\n",
        "In this assignment you will further deepen your understanding of Probabilistic Modeling, Unsupervised Learning, and Reinforcement Learning (RL).\n",
        "\n",
        "## Submission instructions\n",
        "\n",
        "Please write your answers, equations, and code directly in this python notebook and print the final result to pdf (File > Print).\n",
        "Make sure that code has appropriate line breaks such that all code is visible in the final pdf.\n",
        "If necessary, select A3 for the PDF size to prevent content from being clipped.\n",
        "\n",
        "The final pdf must be named name.lastname.pdf and uploaded to the iCorsi website before the deadline expires. Late submissions will result in 0 points.\n",
        "\n",
        "**Also share this notebook (top right corner 'Share') with teaching.idsia@gmail.com during submission.**\n",
        "\n",
        "**Keep your answers brief and respect the sentence limits in each question (answers exceeding the limit are not taken into account)**.\n",
        "\n",
        "Note that there are a total of **140 points in this assignment**.\n",
        "\n",
        "Learn more about python notebooks and formatting here: https://colab.research.google.com/notebooks/welcome.ipynb\n",
        "\n",
        "## How to get help\n",
        "\n",
        "We encourage you to use the tutorials to ask questions or to discuss exercises with other students.\n",
        "However, do not look at any report written by others or share your report with others.\n",
        "Violation of that rule will result in 0 points for all students involved. For further questions you can contact the respective TA:\n",
        "\n",
        "- Probabilistic Modeling: louis@idsia.ch\n",
        "- Unsupervised Learning: anand@idsia.ch\n",
        "- Reinforcement Learning: dylan.ashley@idsia.ch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpyRFlxOqRJH"
      },
      "source": [
        "## 1 Probabilistic Modeling (20p)\n",
        "\n",
        "If $X \\sim \\text{Rayleigh}(\\lambda)$ is a Rayleigh random variable then its probability density function is given by \n",
        "\\begin{equation*}\n",
        "    p(x; \\lambda) = \\frac{2x}{\\lambda} e^{-\\frac{x^2}{\\lambda}} \\ \\ \\text{for} \\ \\ x \\geq 0.\n",
        "\\end{equation*}\n",
        "Let $(X_1, X_2, \\ldots, X_N)$ be a random sample from the Rayleigh distribution with parameter $\\lambda$ and let $\\mathcal{D} = (x_1, x_2, \\ldots, x_N)$ be its realization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubdJF8bbqRJI"
      },
      "source": [
        "### Question 1.1 (4p)\n",
        "\n",
        "Derive the likelihood function of $\\lambda$ given $\\mathcal{D}$.  Include all intermediate steps. Simplify sums and products.\n",
        "\n",
        "$$\\mathcal{L}(\\lambda; \\mathcal{D}) = ?$$\n",
        "\n",
        "---\n",
        "$$\\mathcal\n",
        " L(\\lambda, D)=\\prod _{i=1}^{n}(\\frac{2x_{i}}{\\lambda})e^{\\frac{-x_{i}^{2}}{\\lambda}}=(\\frac{2}{\\lambda})^{n}e^{-\\sum_{i=1}^{n}{x_{i}^{2}/\\lambda}}\\prod_{i=1}^{n} {x_{i}}\n",
        " $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "id9Gzfn-qRJI"
      },
      "source": [
        "### Question 1.2 (2p)\n",
        "\n",
        "Derive the log-likelihood. Include all intermediate steps.\n",
        "\n",
        "$$\\ln \\mathcal{L}(\\lambda; \\mathcal{D}) = ?$$\n",
        "\n",
        "---\n",
        "$$\\mathcal\n",
        " \\ln{L(\\lambda, D)}=\\ln{(\\prod _{i=1}^{n}(\\frac{2x_{i}}{\\lambda})e^{\\frac{-x_{i}^{2}}{\\lambda}}=(\\frac{2}{\\lambda})^{n}e^{-\\sum_{i=1}^{n}{x_{i}^{2}/\\lambda}}\\prod_{i=1}^{n} {x_{i}})}\\\\=\\ln{\\frac{2}{\\lambda}^{n}}+ln{e^{\\frac{-x_{i}^{2}}{\\lambda}}}+ln{\\prod _{i=1}^{n}{x_{i}}}=n\\ln{2}-n\\ln{\\lambda}-\\lambda\\sum_{i=1}^{n}{x_{i}^{2}}+\\sum_{i=1}^{n}{\\ln{x_{i}}}\n",
        " $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcEbEDS4qRJJ"
      },
      "source": [
        "### Question 1.3 (6p)\n",
        "\n",
        "Derive the maximum likelihood estimate for $\\lambda$. Include all intermediate steps.\n",
        "\n",
        "---\n",
        "\n",
        "$$\\frac{\\partial }{\\partial \\lambda} \\ln{L(\\lambda, D)}=0\\; we \\;have\\; to\\; set\\; this\\;to\\; zero\\;$$\n",
        "\n",
        "$$\\frac{\\partial }{\\partial \\lambda}{nln2}-\\frac{\\partial }{\\partial \\lambda}{nln{\\lambda}}-\\sum_{i=1}^{n}\\frac{\\partial }{\\partial \\lambda}(\\frac{1}{\\lambda})+\\frac{\\partial }{\\partial \\lambda}(\\sum_{i}^{n}{\\ln{x_{i}^{2}}})=0$$\n",
        "\n",
        "$$-\\frac{n}{\\lambda}+\\sum{x_{i}^{2}}(\\frac{1}{\\lambda^{2}})=0$$\n",
        "\n",
        "$$-n+\\sum{x_{i}^{2}}(\\frac{1}{\\lambda})=0$$\n",
        "\n",
        "$$\\lambda=\\frac{1}{n}\\sum{x_{i}^{2}}$$\n",
        "\n",
        "\n",
        "we must compute the second derivative to be sure that\n",
        "we found the local maximum and not the local minimum of our function\n",
        "$$\\frac{\\partial^2 }{\\partial \\lambda^2}\\ln{L(\\lambda, D)}=\\frac{\\partial }{\\partial \\lambda}[-\\frac{n}{\\lambda}+\\sum{x_{i}^{2}}(\\frac{1}{\\lambda^{2}})]$$\n",
        "$$=\\frac{n}{\\lambda^{2}}-2\\sum_{i=1}^{n}{\\frac{x_{i}^{2}}{\\lambda^{3}}}$$\n",
        "$$=\\frac{[n\\lambda-2\\sum_{i=1}^{n}x_{i}^{2}]}{\\lambda^{3}}$$\n",
        "$$=\\frac{-n^{3}}{(\\sum{x_{i}^{2}})^{2}}<0\\;\n",
        "with\n",
        "\\sum{x_{i}^{2}} \\neq 0$$\n",
        "we can underline that the last term is less than zero, so we have found the maximum.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUnIB-RgqRJJ"
      },
      "source": [
        "### Question 1.4 (8p)\n",
        "\n",
        "Now, suppose we have an inverse gamma prior with parameters $\\alpha$ and $\\beta$ on $\\lambda$, i.e. the probability density function of the prior is\n",
        "\\begin{equation*}\n",
        "    p(\\lambda; \\alpha, \\beta) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\lambda^{-(\\alpha + 1)} e^{-\\beta / \\lambda}\n",
        "\\end{equation*}\n",
        "where $\\Gamma(\\cdot)$ is the gamma function defined as $\\Gamma(t) = \\int_0^\\infty u^{t-1} e^{-u} \\mathrm{d}u$.\n",
        "\n",
        "What is the maximum a posteriori estimate for $\\lambda$? Include all intermediate steps.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "from Bayes Theorem: $$P(\\lambda\\mid D)=\\frac{P(\\lambda\\mid D)P(\\lambda)}{P(D)}$$\\\\}\n",
        "\n",
        "$$\\lambda=argmax(P(\\lambda\\mid D)=(\\frac{P(\\lambda\\mid D)P(\\lambda)}{P(D)})\\\\$$\n",
        "$$ln(\\frac{P(\\lambda\\mid D)P(\\lambda)}{P(D)})=ln(P(\\lambda\\mid D))-ln(P(D))+P(\\lambda)=*\\\\$$\n",
        "\n",
        "Now we have to take the first derivative\n",
        "$$\\frac{\\partial }{\\partial \\lambda}[*]$$\n",
        " but the term $$ln(P(D))$$\n",
        "does not depend on lambda, so the derivative respect to lambda is =0\n",
        "we know from 1.2 what is $$ln(P(\\lambda\\mid D))$$\n",
        "\n",
        "$$\\frac{\\partial }{\\partial \\lambda}(nln(\\lambda)-ln(\\lambda)-\\sum_{i=1}^{n}(\\frac{x_{i}^{2}}{\\lambda})+ \\sum(ln(x_{i})))+\\frac{\\partial }{\\partial \\lambda}ln(\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\lambda^{-(\\alpha + 1)} e^{-\\beta / \\lambda})$$\n",
        "$$=\\frac{-n}{\\lambda}+\\frac{\\sum(x_{i}^{2})}{\\lambda^{2}}-\\frac{(\\alpha+1)}{\\lambda}+\\frac{\\beta}{\\lambda^{2}}=0$$\n",
        "$$- n +\\frac{1}{\\lambda} \\sum (x_{i}^{2})-(\\alpha+1) +\\frac{\\beta}{\\lambda}=0$$\n",
        "$$-\\lambda n+\\sum(x_{i}^{2})-\\lambda (\\alpha+1) +\\beta=0$$\n",
        "$$\\lambda (n+ \\alpha +1)= \\sum(x_{i}^2)$$\n",
        "$$\\lambda=\\frac{\\sum(x_{i}^{2}) + \\beta}{n+1+\\alpha}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OjZttfAR2Vo"
      },
      "source": [
        "## 2 Unsupervised Learning (30p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyWHcP4WCRuO"
      },
      "source": [
        "### Question 2.1 (5p)\n",
        "Given a dataset of points in 1-dimension i.e. $X = \\{0, 2, 4, 8, 12 \\}$, initialize the k-Means algorithm with 2 cluster center(s) as $\\mu_1$ = 0 and $\\mu_2$ = 12. Perform 1 iteration (1 E-step and 1 M-step) of the k-Means algorithm with the given cluster initializations.  Compute cluster assignments for all datapoints and cluster centers at the end of the iteration. (Show **all** intermediate computation steps).\n",
        " \n",
        "---\n",
        "$$\\ \n",
        "\\text{We can consider the Euclidean distance defined as }\\\\\n",
        "Distance=D=\\left | p-q_{i} \\right |\\\\ \\text{ where p and q are points; in this case p is the center of one cluster and q(i) is a generic point}\n",
        "$$\n",
        "\n",
        "\\\\\n",
        "\\begin{array}{|c | c |c | c|}\\hline\n",
        "     \\text{ Datapoints } & \\text{ D1 } & \\text{ D2 } &\\text{Cluster}\\\\\\hline\n",
        "     0       & 0           & 12  & C1\\\\\\hline\n",
        "     2     & 2              & 10 & C1\\\\\\hline\n",
        "     4    & 4             & 8    & C1  \\\\\\hline\n",
        "     8       & 8        &  4   & C2    \\\\\\hline\n",
        "     12     & 12              &  0 &   C2\\\\\\hline\n",
        "\\end{array} \n",
        "\n",
        "\n",
        "D1 is the distance from the center of the first cluster, D2 from the center of the second cluster. We have to compare these two distances and assign that point to the cluster which has the shortest distance as showed above.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJiP3KxzGBn5"
      },
      "source": [
        "### Question 2.2 (10p)\n",
        "For the k-Means algorithm, given a dataset $X = \\{ x_1, x_2, ... , x_i, ... , x_n \\}$ of $n$ points where $x_i \\in \\mathbb{R}^d$. We want to cluster this into $k$ clusters with centers $\\mu = (\\mu_1, ..., \\mu_k)$. Show that k-Means is guaranteed to converge to a local optimum. Convergence criteria is said to have been met in k-Means when no change of cluster assignment happens for any datapoint between iteration $t$ and $t+1$. *Hint:* Prove that the loss function decreases monotonically in each iteration until convergence (prove it separately for both the E-step and M-step).\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**ANSWER HERE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6uOgFnjIgI8"
      },
      "source": [
        "### Question 2.3 (5p)\n",
        "Find the Eigendecomposition (i.e. eigenvalues and eigenvectors) for \n",
        "\\begin{equation}\n",
        " A = \\begin{bmatrix} \n",
        " 3 & 5 \\\\ 4 & 2 \n",
        " \\end{bmatrix}\n",
        "\\end{equation}\n",
        "\n",
        "---\n",
        "\\begin{equation}\n",
        "(A-\\lambda I)=0\\\\\n",
        "\\begin{pmatrix}\n",
        "3&5 \\\\ 4& 2\\end{pmatrix}\n",
        "-\\begin{pmatrix}\\lambda & 0\\\\\n",
        "0& \\lambda\\end{pmatrix}=0\\\\\n",
        "\\begin{pmatrix}\n",
        "3-\\lambda & 5\\\\ \n",
        " 4&2- \\lambda\n",
        "\\end{pmatrix}=0\\\\\n",
        "(2-\\lambda)^2-20=0\\\\\n",
        "\\lambda^{2}-5 \\lambda-14=0\\\\\n",
        "\\lambda_{1}=7\\, and\\; \\lambda_{2}=-2\\\\\n",
        " \\text{now let's compute the first eigenvector}\\\\ \\end{equation}\n",
        " \\begin{equation}\n",
        "(A-\\lambda_{1} I)v_{1}=0\\\\\n",
        "\\begin{pmatrix}\n",
        "3-7 &5 \\\\\n",
        " 4&2-7 \n",
        "\\end{pmatrix}\\begin{pmatrix}\n",
        "v_{1,1}\\\\v_{1,2}\n",
        "\\end{pmatrix}=\\begin{pmatrix}\n",
        "0\\\\ 0\n",
        "\\end{pmatrix} \\\\\n",
        "v_{1,2}=\\frac{4}{5}v_{1,1}\\\\\\\\\n",
        "v_{1}=k_{1}\\begin{pmatrix}\n",
        "1\\\\ \\frac{4}{5}\n",
        "\\end{pmatrix} \\text{where k1 is an arbitrary constant}\\end{equation}\n",
        "I applied the sam mechanism for the second eingenvalue and eigenvector. \n",
        "\\begin{equation}\n",
        "(A-\\lambda_{2} I)v_{2}=0\\\\ \\\\\n",
        "\\begin{pmatrix}\n",
        "3+2 &5 \\\\ \\\\\n",
        " 4&2+2 \n",
        "\\end{pmatrix}\\begin{pmatrix}\n",
        "v_{2,1}\\\\v_{2,2}\n",
        "\\end{pmatrix}=\\begin{pmatrix}\n",
        "0\\\\ 0\n",
        "\\end{pmatrix} \\\\\\\\\\\\\n",
        "v_{2,2}=-v_{2,1}\\\\\\\\\n",
        "v_{2}=k_{2}\\begin{pmatrix}\n",
        "1\\\\ -1\n",
        "\\end{pmatrix}\n",
        "\\end{equation}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sRCavWTL6YN"
      },
      "source": [
        "### Question 2.4 (10p)\n",
        "Prove that a linear projection onto an $M$-dimensional subspace that maximizes the variance of the projected data is defined by the $M$ eigenvectors of the data covariance matrix $S$, corresponding to the $M$ largest eigenvalues. *Hint:* We showed this for the simple 1-D case in the TA session. Now extend this proof for the general case $M < D$ using induction.\n",
        "\n",
        "---\n",
        "\n",
        "**ANSWER HERE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okJpynzHMpA6"
      },
      "source": [
        "## 3 Reinforcement Learning: Markov Decision Processes (32p)\n",
        "\n",
        "Suppose a robot is put in a maze with a long corridor. The\n",
        "corridor is 1 kilometer long and 5 meters wide. The available actions to the robot are moving forward 1 meter, moving backward 1 meter, turning left by 90 degrees and turning right by 90 degrees. If the robot moves and hits the wall, then it will stay in its position and orientation. The robot's goal is to escape from this maze by reaching the end of the long corridor.\n",
        "**Note: the answers in the following questions should not exceed 5 sentences.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rI3AATMANzC_"
      },
      "source": [
        "### Question 3.1 (4p)\n",
        "\n",
        "Assume the robot receives a +1 reward signal for each time step taken in the\n",
        "maze and +1000 for reaching the final goal (the end of the long corridor). Then you train the robot for a while, but it seems it still does not perform well at all for navigating to the end of the corridor in the maze. What is happening? Is there something wrong with the reward function?\n",
        "\n",
        "---\n",
        "\n",
        "The robot does not perform well because it can reach about or over 1000 during the train and then it never will escape from this maze. Moreover, if it gets +1 at each time steps, we will assign a positive reward all the time. As a consequence, it does not learn the way to escape at all. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LC-aYmiFOAEZ"
      },
      "source": [
        "### Question 3.2 (4p)\n",
        "\n",
        "If there is something wrong with the reward function, how could you fix it? If not, how to resolve the training issues?\n",
        "\n",
        "---\n",
        "\n",
        "We can fix this problem just assigning to it a bad rewards at each timestep (for instance -1) or we could also assign the value of zero if it has not escape yet. We can assign a positive reward (as +1) only if it has reached the exit. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzIDv8qoOGuH"
      },
      "source": [
        "### Question 3.3 (2p)\n",
        "\n",
        "The discounted return for a non-episodic task is defined as\n",
        "$$\n",
        "G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\ldots\n",
        "$$\n",
        "where $\\gamma \\in [0, 1]$ is the discount factor.\n",
        "\n",
        "Rewrite the above equation such that $G_t$ is alone on the left hand side and $G_{t+1}$ is on the right hand side.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\\begin{equation}\n",
        "G_t=R_{t+1}+\\gamma(R_{t+2}+R_{t+3}+R_{t+4}+R_{t+5}+R_{t+6}+...)\n",
        "=R_{t+1}+G_{t+1}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTxEimD5O5eA"
      },
      "source": [
        "### Question 3.4 (2p)\n",
        "\n",
        "What is the sufficient condition for this infinite series to be a convergent series?\n",
        "\n",
        "---\n",
        "\n",
        "The sufficent condition for this infinite series to be convergent is that lambda must be less than 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bp30j2csPJkz"
      },
      "source": [
        "### Question 3.5 (5p)\n",
        "\n",
        "Suppose this infinite series is a convergent series, and each reward in the series is a constant of +1. We know the series is bounded, what is a simple formula for this bound ? Write it down without using summation.\n",
        "\n",
        "---\n",
        "\n",
        "**ANSWER HERE**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmDQKWHNPSnx"
      },
      "source": [
        "### Question 3.6 (5p)\n",
        "\n",
        "Let the task be an episodic setting and the robot is running for $T = 5$ time steps. Suppose $\\gamma = 0.9$, and the robot receives rewards along the way $R_1 = −1, R_2 = −0.5, R_3 = 2.5, R_4 = 1, R_5 = 3$. What are the values for $G_0, G_1, G_2, G_3, G_4, G_5$?\n",
        "\n",
        "---\n",
        "\\begin{equation}{\n",
        "G_5=0\\\\\n",
        "G_4=R_5+\\gamma G_5=R_5=3\\\\\n",
        "G_3=R_4+\\gamma G_4=1+0.9*3=3.7\\\\\n",
        "G_2=R_3+\\gamma G_3=2.5+0.9*3.7=5.83\\\\\n",
        "G_1=R_2+\\gamma G_2=-0.5+0.9*5.83=4.747\\\\\n",
        "G_0=R_1+\\gamma G_1=-1+0.9*4.747=3.27\\\\}\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7ZcAEvfQA8f"
      },
      "source": [
        "### Question 3.7 (5p)\n",
        "\n",
        "Suppose each reward in the series is increased by a constant $c$, i.e. $R_t \\leftarrow R_t + c$.\n",
        "Then how does it change $G_t$?\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Since we know that (if the rewards are constant and equal to +1):\n",
        "\n",
        "$$G_t=\\sum_{k=0}^{\\infty }{\\gamma^k}=\\frac{1}{1-\\gamma}$$\n",
        "if the rewards is c is constant and the rewards increase along the time, then the sum becomes :\n",
        "\n",
        "$$G_t=\\sum_{k=0}^{\\infty }{c * \\gamma^k}$$ since c is constant, it does not depend on k:\n",
        "$$c* \\sum_{k=0}^{\\infty }{ \\gamma^k}=\\frac{c*1}{1-\\gamma}=\\frac{c}{1-\\gamma}$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7KPVdhPQBpz"
      },
      "source": [
        "### Question 3.8 (5p)\n",
        "\n",
        "Now consider episodic tasks, and similar to Question 2.7, we add a constant $c$ to each reward, how does it change $G_t$?\n",
        "\n",
        "---\n",
        "\n",
        "Episodic tesks are definited as:\n",
        "$$G_t=\\sum_{k=0}^{T-t-1 }{\\gamma^k R_k}$$\n",
        "If the reward is increased by c at each time step:\n",
        "$$G_t=\\sum_{k=0}^{T-t-1 }{\\gamma^k (R_k +c)}$$\n",
        " by splitting the sum:\n",
        "\n",
        "$$G_t=\\sum_{k=0}^{T-t-1 }{\\gamma^k R_k } +\\sum_{k=0}^{T-t-1 }{\\gamma^k c}=$$\n",
        "the first term is $G_t$ while the second is :\n",
        "$$c\\; \\frac{1- \\gamma^{T-t}}{1- \\gamma}$$\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjOnM7AVQVOB"
      },
      "source": [
        "## 4 Reinforcement Learning: Dynamic Programming (58p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Jm0EyIHQaOp"
      },
      "source": [
        "### Question 4.1 (5p)\n",
        "\n",
        "Write down the Bellman optimality equation for the state value function without using expectation notation, but using probability distributions instead. \n",
        "Define all variables and probability distributions in bullet points.\n",
        "\n",
        "---\n",
        "\n",
        "**ANSWER HERE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0VZcK6LQkUC"
      },
      "source": [
        "### Question 4.2 (5p)\n",
        "\n",
        "Write down the Bellman optimality equation for the state-action value function without using expectation notation, but using probability distributions instead.\n",
        "Define all variables and probability distributions in bullet points.\n",
        "\n",
        "---\n",
        "\n",
        "**ANSWER HERE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tq66sRJeQlE2"
      },
      "source": [
        "### Question 4.3 (15p)\n",
        "\n",
        "Consider a 4x4 gridworld depicted in the following table:\n",
        "\n",
        "![Grid world](https://i.ibb.co/HdSdKJB/image.png)\n",
        "\n",
        "The non-terminal states are $S = \\{1, 2, \\ldots, 14\\}$ and the terminal states are $0, 15$.\n",
        "There are four available actions for each state, that is $A = \\{\\text{up}, \\text{down}, \\text{left}, \\text{right}\\}$.\n",
        "Assume the state transitions are deterministic and all transitions result in a negative reward of −1.\n",
        "If the agent hits the boundary, then its state will remain unchanged, e.g. $p(s=8, r=−1|s=8, a=\\text{left}) = 1$.\n",
        "Note: In this exercise, we assume the policy is a deterministic\n",
        "function and it initially maps each state to an arbitrary action.\n",
        "\n",
        "\n",
        "Manually run the policy iteration algorithm for one outer iteration. Use the in-place policy iteration algorithm (directly using the updated values for the next value update).\n",
        "To be specific, run the policy evaluation with a single pass through the states (16 equations, not until convergence) and one time policy improvement.\n",
        "Assume the initial state value for all 16 cells is 0.0 and the policy initially always outputs the 'left' action.\n",
        "Write down the equations and detailed numerical computations for the updated values of each cell.\n",
        "Use a discount factor $\\gamma = 0.5$.\n",
        "Write down the policy after policy improvement.\n",
        "\n",
        "Read more about this in Sutton & Barto's book http://www.incompleteideas.net/book/ebook/node43.html\n",
        "\n",
        "---\n",
        "\n",
        "**ANSWER HERE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciourK1wQo-3"
      },
      "source": [
        "### Question 4.4 (12p)\n",
        "\n",
        "Implement the environment as described in the code skeleton below.\n",
        "Come up with your own solution and do not copy the code from a third party source.\n",
        "\n",
        "Then test your implementation of GridWorld using the implementation of policy iteration provided below. Run the code multiple times. Do you always end up with the same policy? Why? (max 4 sentences)\n",
        "\n",
        "---\n",
        "\n",
        "I end up with the same policy becuase it is a deterministic approach. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3gmuwlmdyO4"
      },
      "source": [
        "#### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkzicBh-I3dU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import itertools\n",
        "\n",
        "np.set_printoptions(precision=3, linewidth=180)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DT_bNpgqd2mM"
      },
      "source": [
        "\n",
        "#### Defining the problem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNBoBp3PJC0C"
      },
      "outputs": [],
      "source": [
        "class GridWorld:\n",
        "\n",
        "    UP = 0\n",
        "    DOWN = 1\n",
        "    LEFT = 2\n",
        "    RIGHT = 3\n",
        "    table=np.arange(0, 16)\n",
        "    table=table.reshape([4,4])\n",
        "    #print(\"grid: \\n\\n\", table)\n",
        "    def __init__(self, side=4):\n",
        "        self.side = side\n",
        "        # -------------------------\n",
        "        # Define integer states, actions, and final states as specified\n",
        "        # in the problem description\n",
        "    \n",
        "        # TODO insert code here\n",
        "        self.actions = np.array([self.DOWN, self.LEFT, self.RIGHT, self.UP])\n",
        "        self.states = np.arange(0, 16)\n",
        "        self.finals = np.array([0, 15])\n",
        "\n",
        "        # -------------------------\n",
        "        self.actions_repr = np.array(['↑', '↓', '←', '→'])\n",
        "\n",
        "    def _is_terminal(self, s):\n",
        "        # -------------------------\n",
        "        # Return True if s is a terminal state and False otherwise\n",
        "        return(s==15 or s==0)\n",
        "        # TODO insert code here\n",
        "\n",
        "        # -------------------------\n",
        "\n",
        "    def _next_state(self, s, a):\n",
        "        # -------------------------\n",
        "        # Returns the next state of the environment if action a were taken\n",
        "        # while in state s\n",
        "        \n",
        "        \n",
        "        for i in self.table[0]:\n",
        "            if(a==self.UP and s==i):\n",
        "              return s\n",
        "        \n",
        "        for i in self.table[3]:\n",
        "            if(a==self.DOWN and s==i):\n",
        "              return s\n",
        "        \n",
        "        for i in self.table:\n",
        "            j=i[0]\n",
        "            if(a==self.LEFT and s==j):\n",
        "              return s\n",
        "        \n",
        "        \n",
        "        for i in self.table:\n",
        "          j=i[3]\n",
        "          if(a==self.RIGHT and s==j):\n",
        "            return s\n",
        "        \n",
        "        for i in range(0,4):\n",
        "            for j in range(0,4):\n",
        "              if(a==self.UP and s==self.table[i][j]):\n",
        "                return self.table[i-1][j]\n",
        "              elif(a==self.DOWN and s==self.table[i][j]):\n",
        "                return self.table[i+1][j]\n",
        "              elif(a==self.LEFT and s==self.table[i][j]):\n",
        "                return self.table[i][j-1]\n",
        "              elif(a==self.RIGHT and s==self.table[i][j]):\n",
        "                return self.table[i][j+1]\n",
        "            \n",
        "        # TODO insert code here\n",
        "\n",
        "        # -------------------------\n",
        "\n",
        "    def _reward(self, s, s_next, a):\n",
        "        # -------------------------\n",
        "        # Return the reward for the given transition as specified\n",
        "        # in the problem description\n",
        "        if(self._is_terminal(s)):\n",
        "          return 0\n",
        "        else:\n",
        "          return -1\n",
        "        # TODO insert code here\n",
        "\n",
        "        # -------------------------\n",
        "    \n",
        "    def reset(self):\n",
        "        # -------------------------\n",
        "        # Set the internal state of the environment to be sampled uniformly\n",
        "        # at random from the set of non-terminal states and return the state\n",
        "        \n",
        "        # TODO insert code here\n",
        "        self.s = np.random.uniform(1,15)\n",
        "\n",
        "        # -------------------------\n",
        "    \n",
        "    def step(self, a):\n",
        "        old_s=self.s\n",
        "        self.s=self._next_state(self.s,a)\n",
        "        T=self._is_terminal(self.s)\n",
        "        r=self._reward(old_s, self.s, a)\n",
        "        return \n",
        "\n",
        "\n",
        "        # -------------------------\n",
        "        # Advances the environment one step using action a and returns s, r, T\n",
        "        # where s is the next state, r is the reward, and T is a boolean saying\n",
        "        # whether the episode is done or not\n",
        "        \n",
        "        # TODO insert code here\n",
        "\n",
        "        # -------------------------\n",
        "\n",
        "    def print_policy(self, policy):\n",
        "        P = np.array(policy).reshape(self.side, self.side)\n",
        "        print(self.actions_repr[P])\n",
        "  \n",
        "    def print_values(self, values):\n",
        "        V = np.array(values).reshape(self.side, self.side)\n",
        "        print(V)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrUMxh-qd5u0"
      },
      "source": [
        "#### Policy iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1DOXcH5J0NR"
      },
      "outputs": [],
      "source": [
        "def transition_prob(s, s_next, a):\n",
        "    return 1 if problem._next_state(s, a) == s_next else 0\n",
        "\n",
        "\n",
        "def eval_policy(problem, policy, value, gamma=0.9, theta=0.01):\n",
        "    p = transition_prob\n",
        "    r = problem._reward\n",
        "\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for s in problem.states:\n",
        "            v = value[s]\n",
        "            value[s] = np.sum(\n",
        "                [\n",
        "                    p(s, next_s, policy[s])\n",
        "                    * (r(s, next_s, policy[s]) + gamma * value[next_s])\n",
        "                    for next_s in problem.states\n",
        "                ]\n",
        "            )\n",
        "            delta = max(delta, abs(v - value[s]))\n",
        "\n",
        "        if delta < theta:\n",
        "            return value\n",
        "\n",
        "\n",
        "def improve_policy(problem, policy, value, gamma=0.9):\n",
        "    p = transition_prob\n",
        "    r = problem._reward\n",
        "\n",
        "    stable = True\n",
        "    for s in problem.states:\n",
        "        actions = problem.actions\n",
        "\n",
        "        b = policy[s]\n",
        "        policy[s] = actions[\n",
        "            np.argmax(\n",
        "                [\n",
        "                    np.sum(\n",
        "                        [\n",
        "                            p(s, next_s, a) * (r(s, next_s, a) + gamma * value[next_s])\n",
        "                            for next_s in problem.states\n",
        "                        ]\n",
        "                    )\n",
        "                    for a in actions\n",
        "                ]\n",
        "            )\n",
        "        ]\n",
        "        if b != policy[s]:\n",
        "            stable = False\n",
        "\n",
        "    return stable\n",
        "\n",
        "\n",
        "def policy_iteration(problem, gamma=0.9, theta=0.01):\n",
        "    # Initialize a random policy\n",
        "    policy = np.array([np.random.choice(problem.actions) for s in problem.states])\n",
        "    print(\"Initial policy\")\n",
        "    problem.print_policy(policy)\n",
        "    # Initialize values to zero\n",
        "    values = np.zeros_like(problem.states, dtype=np.float32)\n",
        "\n",
        "    # Run policy iteration\n",
        "    stable = False\n",
        "    for i in itertools.count():\n",
        "        print(f\"Iteration {i}\")\n",
        "        values = eval_policy(problem, policy, values, gamma, theta)\n",
        "        problem.print_values(values)\n",
        "        stable = improve_policy(problem, policy, values, gamma)\n",
        "        problem.print_policy(policy)\n",
        "        if stable:\n",
        "            break\n",
        "\n",
        "    return policy, values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QqdoNw97mcEA",
        "outputId": "e2960fd3-187c-42ee-e6cb-d6005efebae9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial policy\n",
            "[['↑' '↓' '→' '←']\n",
            " ['↑' '↑' '←' '↑']\n",
            " ['↓' '↓' '↑' '←']\n",
            " ['↑' '↑' '↓' '↑']]\n",
            "Iteration 0\n",
            "[[ 0.    -2.    -2.    -2.   ]\n",
            " [-1.    -2.    -2.    -2.   ]\n",
            " [-2.    -2.    -2.    -2.   ]\n",
            " [-2.    -2.    -1.992 -1.   ]]\n",
            "[['←' '←' '←' '←']\n",
            " ['↑' '←' '↑' '↑']\n",
            " ['↑' '←' '↓' '↓']\n",
            " ['↑' '→' '→' '↓']]\n",
            "Iteration 1\n",
            "[[ 0.    -1.    -1.5   -1.75 ]\n",
            " [-1.    -1.5   -1.75  -1.875]\n",
            " [-1.5   -1.75  -1.508 -1.008]\n",
            " [-1.75  -1.508 -1.008 -0.008]]\n",
            "[['←' '←' '←' '←']\n",
            " ['↑' '←' '←' '↓']\n",
            " ['↑' '←' '↓' '↓']\n",
            " ['↑' '→' '→' '↓']]\n",
            "Iteration 2\n",
            "[[ 0.    -1.    -1.5   -1.75 ]\n",
            " [-1.    -1.5   -1.75  -1.502]\n",
            " [-1.5   -1.75  -1.502 -1.002]\n",
            " [-1.75  -1.502 -1.002 -0.002]]\n",
            "[['←' '←' '←' '←']\n",
            " ['↑' '←' '←' '↓']\n",
            " ['↑' '←' '↓' '↓']\n",
            " ['↑' '→' '→' '↓']]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([2, 2, 2, 2, 0, 2, 2, 1, 0, 2, 1, 1, 0, 3, 3, 1]),\n",
              " array([ 0.   , -1.   , -1.5  , -1.75 , -1.   , -1.5  , -1.75 , -1.502, -1.5  , -1.75 , -1.502, -1.002, -1.75 , -1.502, -1.002, -0.002], dtype=float32))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# Run the below code, please include the output in your submission\n",
        "problem = GridWorld()\n",
        "policy_iteration(problem, gamma=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hRFm6Zm6YI1"
      },
      "source": [
        "### Question 4.5 (5p)\n",
        "\n",
        "Let's run policy iteration with $\\gamma = 1$. Describe what is happening. Why is this the case? Give an example. What is $\\gamma$ trading off and how does it affect policy iteration? (max 8 sentences)\n",
        "\n",
        "---\n",
        "\n",
        "With $\\gamma$ equal to one it does not converge because we have an infinite sum of non-zero terms as this case. It shall be set less than 1.\\\\\n",
        "We can have 2 cases: the first one when we set: $\\gamma$ to zero and so the agent tries to maximize the immidiate reward (greedy/myopic), and the second case when we set $\\gamma$ less than 1. Indeed, with $\\gamma$ less than 1 we consider much more the future rewards. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rAQ1K_u6qtH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "outputId": "50ae4620-f81e-40d1-cf79-da77850220cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial policy\n",
            "[['←' '↓' '←' '↓']\n",
            " ['→' '↑' '↑' '↑']\n",
            " ['→' '↑' '↑' '←']\n",
            " ['↓' '→' '→' '↑']]\n",
            "Iteration 0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-dc3c4e33788e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpolicy_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproblem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-30-fe2249eb40fc>\u001b[0m in \u001b[0;36mpolicy_iteration\u001b[0;34m(problem, gamma, theta)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Iteration {i}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproblem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mproblem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mstable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimprove_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproblem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-fe2249eb40fc>\u001b[0m in \u001b[0;36meval_policy\u001b[0;34m(problem, policy, value, gamma, theta)\u001b[0m\n\u001b[1;32m     15\u001b[0m                     \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                     \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_s\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mnext_s\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproblem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m                 ]\n\u001b[1;32m     19\u001b[0m             )\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2241\u001b[0m     return _wrapreduction(a, np.add, 'sum', axis, dtype, out, keepdims=keepdims,\n\u001b[0;32m-> 2242\u001b[0;31m                           initial=initial, where=where)\n\u001b[0m\u001b[1;32m   2243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_wrapreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m     passkwargs = {k: v for k, v in kwargs.items()\n\u001b[1;32m     72\u001b[0m                   if v is not np._NoValue}\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "policy_iteration(problem, gamma=1.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BiSlnccwuVDM"
      },
      "source": [
        "### Question 4.6 (16p)\n",
        "\n",
        "Implement Q-learning using the code skeleton below.\n",
        "Come up with your own solution and do not copy the code from a third party source.\n",
        "Then execute the block to show that your solution reached when $\\gamma = 0.5$ is optimal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLBufZOFvTOJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "02cd48f6-d7f5-4a23-cad6-93229b7362e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "None\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-d165332d6866>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;31m# -------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Perform one step of Q-learning here using sa_values to store\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "problem = GridWorld()\n",
        "GAMMA = 0.5\n",
        "sa_values = np.zeros((len(problem.states), len(problem.actions)), dtype=float)\n",
        "\n",
        "def epsilon_greedy(a_values, epsilon):\n",
        "    # -------------------------\n",
        "    # This function takes a list a_values where each index i corresponds\n",
        "    # to an estimate of the value of action i and then performs\n",
        "    # epsilon-greedy action selection to sample and return an action\n",
        "    \n",
        "    # TODO insert code here\n",
        "    pass\n",
        "    # -------------------------\n",
        "\n",
        "for i in range(100000):\n",
        "    s = problem.reset()\n",
        "    print(s)\n",
        "    done = False\n",
        "    while not done:\n",
        "        # -------------------------\n",
        "        # Perform one step of Q-learning here using sa_values to store\n",
        "        # the action-value estimates and epsilon_greedy to perform the\n",
        "        # action selection\n",
        "        #\n",
        "        # Play around to find a good step size\n",
        "\n",
        "        # TODO insert code here\n",
        "        pass\n",
        "        # -------------------------\n",
        "\n",
        "\n",
        "optimal_policy_state_values = policy_iteration(problem, gamma=GAMMA)[1]\n",
        "learned_policy_state_values = np.max(sa_values, axis=1)\n",
        "\n",
        "print('Optimal Policy State Values:')\n",
        "print(optimal_policy_state_values)\n",
        "print('Learned Policy State Values:')\n",
        "print(learned_policy_state_values)\n",
        "print('Root Mean Squared Value Error: {0:.8f}'.format(\n",
        "    np.sqrt(np.mean(np.square(optimal_policy_state_values - learned_policy_state_values)))))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "ML_AS2_filippo_casari.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}